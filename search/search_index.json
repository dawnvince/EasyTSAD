{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>EasyTSAD is a suite to facilitate the quick implementation and iteration of your time series anomaly detection algorithms. You can also easily develop a new set of evaluation metrics based on this suite and assess them against baseline methods. </p> <p>We welcome you to send the algorithm code implemented based on this suite to our email. We will integrate your method into the algorithm library, making it convenient for researchers and practitioners to utilize your approach or evaluation protocol.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#for-algorithm-researches","title":"For Algorithm Researches","text":"<ul> <li>Flexible interface for algorithm implementation, training and testing new algorithms on one-by-one, all-in-one and zero-shot training schemas.</li> <li>Full pipeline for load dataset, run experiments, do evaluations and analysis(e.g. plots and compares anomaly scores, or generate CSVs for intuitional comparison) the performance of methods.</li> <li>Diversity evaluation protocols for preformance evaluations.</li> </ul>"},{"location":"#for-evaluation-researches","title":"For Evaluation Researches","text":"<ul> <li>Flexible interface for evaluation protocol implementation based on anomaly scores and ground truth labels.</li> <li>Easily perform evaluations on existing methods according to your protocol. </li> <li>Evaluation based on offline scores of methods, which generated by merely once training and test phase.</li> </ul>"},{"location":"#for-practitioners-of-community-or-enterprise","title":"For Practitioners of Community or Enterprise","text":"<ul> <li>Unified and clear Datasets format, easy for introduction of private datasets.</li> <li>Easy performance comparison of baselines on your dataset. E.g. Overall performance in CSV format based on protocols suitable for your applications; Plots of all methods on specify curves.</li> <li>Record runtime statistics (e.g. model parameter size, inference time) for performance, cost, and efficiency trade-off.</li> <li>An Evaluation protocol designed for real-time AD scenarios (EasyTSAD.Evaluations.Protocols.EventF1PA, for details please refer to our paper).</li> </ul>"},{"location":"#leaderboard-representation","title":"Leaderboard Representation","text":"<ul> <li>We provide a continuous integrated leaderboard (https://adeval.cstcloud.cn/content/home) based on this suite and make it vivid to show state-of-the-art algorithms rankings based on various training schemas and evaluation protocols. </li> <li>Welcome to provide us your algorithms or evaluation criterion based on this suite by e-mails. We will add it into the leaderboard after checking, running, and obtaining your permission.</li> </ul>"},{"location":"start/","title":"Installation","text":""},{"location":"start/#prerequisites-environment-manager-like-conda-pipenv-or-poetry-is-recommended","title":"Prerequisites (environment manager like conda, pipenv or poetry is recommended)","text":"<ul> <li>python &gt;= 3.9, &lt; 3.13</li> </ul>"},{"location":"start/#using-pip-to-install-the-suite-from-pypi","title":"Using <code>pip</code> to install the suite from Pypi","text":"<pre><code>pip install EasyTSAD\n</code></pre>"},{"location":"start/#additonal-dependencies","title":"Additonal Dependencies","text":"<p>Some built-in algorithms are based on Pytorch 2.0 or Pytorch-lightning 2.0. You may need to install related packages (including but not limited to pytorch, pytorch-lightning, torchinfo, torch_optimizer) if you want to run the baselines.</p>"},{"location":"start/#prepare-datasets","title":"Prepare datasets","text":""},{"location":"start/#use-default-datasets","title":"Use default datasets","text":"<p>Original datasets can be downloaded from https://wait-to-be-published.  The directory structure of the dataset is shown as follows:</p> <pre><code>datasets\n\u2514\u2500\u2500 UTS\n    \u251c\u2500\u2500 dataset_1\n    \u2502   \u251c\u2500\u2500 time_series_1\n    \u2502   \u2502   \u251c\u2500\u2500 train.npy (training set, 1-D ndarray, necessary)\n    \u2502   \u2502   \u251c\u2500\u2500 test.npy (test set, 1-D ndarray, necessary)\n    \u2502   \u2502   \u251c\u2500\u2500 train_label.npy (labels of training set, 1-D ndarray, neccessary)\n    \u2502   \u2502   \u251c\u2500\u2500 test_label.npy (labels of test set, 1-D ndarray, necessary)\n    \u2502   \u2502   \u251c\u2500\u2500 train_timestamp.npy (timestamps of training set, 1-D ndarray, optional)\n    \u2502   \u2502   \u251c\u2500\u2500 test_timestamp.npy (timestamps of test set, 1-D ndarray, optional)\n    \u2502   \u2502   \u2514\u2500\u2500 info.json (some additonal information, json, optional)\n    \u2502   \u2502\n    \u2502   \u251c\u2500\u2500 time_series_2\n    \u2502   \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 dataset_2\n    \u2514\u2500\u2500 ...\n\n\n</code></pre> <p>The file <code>info.json</code> contains the information like:</p> <pre><code>{\n    \"intervals\": 300,\n    \"training set anomaly ratio\": 0.00148,\n    \"testset anomaly ratio\": 0.00808,\n    \"total anomaly ratio\": 0.00478\n}\n</code></pre>"},{"location":"start/#add-your-datasets","title":"Add your datasets","text":"<p>Preprocess your dataset to satisfy the above structure and format. Files labeled \"necessary\" must be offered. Then put it under the <code>datasets/UTS/</code> path. </p>"},{"location":"start/#usage","title":"Usage","text":"<p>Examples of how to use the suite can be find here, including: - run baselines with/without customized config files; - implement your new algorithm with/without config files; - implement your new evaluation protocol and evaluate the baselines; - generate CSV including the overall performance of all trained methods; - aggregate all methods' anomaly scores into one plot.</p>"},{"location":"start/#an-example-that-implements-a-new-method","title":"An example that implements a new method.","text":""},{"location":"start/#prepare-a-global-config-toml-file-if-not-provided-the-default-configuration-will-be-applied","title":"Prepare a global config toml file. If not provided, the default configuration will be applied:","text":"<pre><code># One example of GlobalCfg.toml. \n# For more details please refer to the default configuration.\n# The new items will overwrite the default ones.\n[DatasetSetting]\n train_proportion = 1 # Using the last x% of the training set as the new training set. 1 means use the full training set.\n valid_proportion = 0.2 # The proportion of the validation set to the new training set.\n</code></pre>"},{"location":"start/#define-the-controller","title":"Define the Controller","text":"<pre><code>from typing import Dict\nimport numpy as np\nfrom EasyTSAD.Controller import TSADController\n\n# if cfg_path is None, using default configuration\ngctrl = TSADController(cfg_path=\"/path/to/GlobalCfg.toml\")\n</code></pre>"},{"location":"start/#load-dataset-configurations","title":"Load Dataset configurations","text":""},{"location":"start/#option-1-load-certain-time-series-in-one-dataset","title":"Option 1: Load certain time series in one dataset:","text":"<pre><code># Specify certain curves in one dataset, \n# e.g. AIOPS 0efb375b-b902-3661-ab23-9a0bb799f4e3 and ab216663-dcc2-3a24-b1ee-2c3e550e06c9\ngctrl.set_dataset(\n    dataset_type=\"UTS\",\n    dirname=\"/path/to/datasets\", # The path to the parent directory of \"UTS\"\n    datasets=\"AIOPS\",\n    curve_names=[\n        \"0efb375b-b902-3661-ab23-9a0bb799f4e3\",\n        \"ab216663-dcc2-3a24-b1ee-2c3e550e06c9\"\n    ]\n)\n</code></pre>"},{"location":"start/#option-2-load-all-time-series-in-certain-datasets","title":"Option 2: Load all time series in certain datasets:","text":"<pre><code># Use all curves in datasets:\ndatasets = [\"AIOPS\", \"Yahoo\"]\ngctrl.set_dataset(\n    dataset_type=\"UTS\",\n    dirname=\"/path/to/datasets\", # The path to the parent directory of \"UTS\"\n    datasets=datasets,\n)\n</code></pre>"},{"location":"start/#implement-your-algorithm-inherit-from-class-basemethod","title":"Implement your algorithm (inherit from class BaseMethod):","text":"<p>The following class <code>YourAlgo</code> just provides a skeleton, where you should implement several functions.  - The Spot instance will help you understand how to implement a statistic model; - The ARLinear instance will help you understand how to implement a learning-based model (Implemented using PyTorch);</p> <pre><code>from EasyTSAD.Methods import BaseMethod\nfrom EasyTSAD.DataFactory import TSData\n\nclass YourAlgo(BaseMethod):\n    def __init__(self, hparams) -&gt; None:\n        super().__init__()\n        self.__anomaly_score = None\n        self.param_1 = hparams[\"param_1\"]\n\n    def train_valid_phase(self, tsTrain: TSData):\n        ...\n\n    def test_phase(self, tsData: TSData):\n        result = ... \n        self.__anomaly_score = result\n\n    def train_valid_phase_all_in_one(self, tsTrains: Dict[str, TSData]):\n        # used for all-in-one and zero-shot mode\n        ...\n\n    def anomaly_score(self) -&gt; np.ndarray:\n        return self.__anomaly_score\n\n    def param_statistic(self, save_file):\n        pass\n\n</code></pre>"},{"location":"start/#do-experiments-for-your-algorithm","title":"Do Experiments for your algorithm","text":"<p>We offer two options for algorithm setting configuration:  - use config file; - specify the parameters in functions. </p> <p>Note: Parameters defined within a function take higher priority than those specified in the configuration file.</p>"},{"location":"start/#option-1-use-config-file-for-methods-recommended","title":"Option 1: Use config file for methods (Recommended)","text":"<ul> <li>Prepare a toml file, which is a subset of Example.toml, for example:</li> </ul> <pre><code># YourAlgo.toml\n[Data_Params]\n preprocess = \"z-score\" \n[Model_Params.Default]\n param_1 = false\n</code></pre> <ul> <li>Load YourAlgo and the config file:</li> </ul> <pre><code>training_schema = \"one_by_one\"\nmethod = \"YourAlgo\"  # string of your algo class\n\n# run models\ngctrl.run_exps(\n    method=method,\n    training_schema=training_schema,\n    cfg_path=\"path/to/YourAlgo.toml\"\n)\n</code></pre>"},{"location":"start/#option-2-specify-the-parameters-in-functions","title":"Option 2: Specify the parameters in functions","text":"<pre><code>gctrl.run_exps(\n    method=method,\n    training_schema=training_schema,\n    hparams={\n        \"param_1\": False,\n    },\n    preprocess=\"z-score\", \n)\n</code></pre> <p>The Score Results can be founded in path <code>workspace/Results/Scores</code>, and the runtime information can be founded in path <code>workspace/Results/RunTime</code></p>"},{"location":"start/#perform-evaluations-based-on-the-saved-scores","title":"Perform evaluations (Based on the saved scores)","text":"<pre><code>from EasyTSAD.Evaluations.Protocols import EventF1PA, PointF1PA\n# Specifying evaluation protocols\ngctrl.set_evals(\n    [\n        PointF1PA(),\n        EventF1PA(),\n        EventF1PA(mode=\"squeeze\")\n    ]\n)\n\ngctrl.do_evals(\n    method=method,\n    training_schema=training_schema\n)\n\n</code></pre> <p>The Evaluation Results can be founded in path <code>workspace/Results/Evals</code></p>"},{"location":"start/#plot-the-anomaly-scores-for-each-time-series","title":"Plot the anomaly scores for each time series","text":"<pre><code>gctrl.plots(\n    method=method,\n    training_schema=training_schema\n)\n</code></pre> <p>The Plot Results can be founded in path <code>workspace/Results/Plots/score_only</code></p>"},{"location":"API/Evaluation/","title":"Interfaces","text":""},{"location":"API/Evaluation/#Evaluations.EvalInterface","title":"EvalInterface","text":"<p>             Bases: <code>object</code></p> <p>The EvalInterface is an abstract base class that defines the interface for evaluation metrics in a generic evaluation system. It serves as a blueprint for concrete evaluation metric classes that implement specific evaluation logic.</p> <p>Methods:</p> Name Description <code>- calc</code> <p>Abstract method that calculates the evaluation metric based on the provided scores, labels, and margins parameters. It returns an instance of a class that inherits from MetricInterface.</p> <code>- get_name</code> <p>Abstract method that returns the name of the evaluation metric. Concrete classes implementing this interface should provide their own implementation of this method.</p>"},{"location":"API/Evaluation/#Evaluations.EvalInterface.calc","title":"calc  <code>abstractmethod</code>","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Calculates the evaluation metric based on the provided scores, labels, and margins.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>An array of anomaly scores.</p> required <code>labels</code> <code>ndarray</code> <p>An array of ground truth labels.</p> required <code>margins</code> <code>[int(margin - before - anomaly), int(margin - after - anomaly)]</code> <p>You can use margin to prune your evaluations if needed. Defined in https://github.com/dawnvince/EasyTSAD/blob/main/EasyTSAD/Controller/GlobalCfg.toml. </p> required <p>Returns:</p> Name Type Description <code>MetricInterface</code> <code>Type[MetricInterface]</code> <p>An instance of the event detection metric.</p>"},{"location":"API/Evaluation/#Evaluations.MetricInterface","title":"MetricInterface","text":"<p>             Bases: <code>object</code></p> <p>The MetricInterface class is an abstract base class that defines the interface for metrics. It serves as a blueprint for creating subclasses that represent specific metrics. </p> <p>The class includes three abstract methods: add(), avg(), and to_dict(). Subclasses inheriting from this class must implement these methods according to their specific metric calculations and requirements.</p> <p>You should implement the following methods:</p> <pre><code>- add(self, other_metric): This abstract method represents the operation of combining two metrics. It takes another metric object (other_metric) as a parameter and is responsible for adding its values to the current metric.\n\n- avg(self): This abstract method calculates the average value of the metric. It should be implemented by subclasses to compute the average based on the accumulated values.\n\n- to_dict(self): This abstract method converts the metric object into a dictionary representation. It should return a dictionary containing the metric's values and any additional information needed for representation or storage.\n</code></pre>"},{"location":"API/Evaluation/#Evaluations.MetricInterface.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(other_metric)\n</code></pre> <p>This abstract method represents the operation of combining two metrics. It takes another metric object (other_metric) as a parameter and is responsible for adding its values to the current metric.</p>"},{"location":"API/Evaluation/#Evaluations.MetricInterface.avg","title":"avg  <code>abstractmethod</code>","text":"<pre><code>avg()\n</code></pre> <p>This abstract method calculates the average value of the metric. It should be implemented by subclasses to compute the average based on the accumulated values.</p>"},{"location":"API/Evaluation/#Evaluations.MetricInterface.to_dict","title":"to_dict  <code>abstractmethod</code>","text":"<pre><code>to_dict()\n</code></pre> <p>This abstract method converts the metric object into a dictionary representation. It should return a dictionary containing the metric's values and any additional information needed for representation or storage.</p>"},{"location":"API/Method/","title":"The built-in methods","text":""},{"location":"API/Method/#prediction-based","title":"Prediction-based","text":"Class Name Description Ref. AR AutoRegression implemented by a torch linear (using first order difference) Robust regression and outlier detection LSTMADalpha LSTMAD in a seq2seq manner Long Short Term Memory Networks for Anomaly Detection in Time Series LSTMADbeta LSTMAD in a multi-step prediction manner Long Short Term Memory Networks for Anomaly Detection in Time Series"},{"location":"API/Method/#reconstruction-based","title":"Reconstruction-based","text":"Class Name Description Ref. AE AutoEncoder Sparse autoencoder EncDecAD Combine LSTM and AE LSTM-based encoder-decoder for multi- sensor anomaly detection SRCNN Time-series anomaly detection service at microsoft Amomaly Transformer Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy TimesNet TIMESNET: TEMPORAL 2D-VARIATION MODELING FOR GENERAL TIME SERIES ANALYSIS"},{"location":"API/Method/#vae-based","title":"VAE-based","text":"Class Name Description Ref. Donut Unsupervised anomaly 1032 detection via variational auto-encoder for seasonal kpis in web applications FCVAE UnderReview"},{"location":"API/Method/#others","title":"Others","text":"Class Name Description Ref. TFAD TFAD: A decomposition time series anomaly detection architecture with time-frequency analysis"},{"location":"API/Protocols/","title":"Protocols","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventDetect","title":"EventDetect","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventDetect.EventDetect","title":"EventDetect","text":"<pre><code>EventDetect()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using the UCR detection protocol to evaluate the models. As there is only one anomaly segment in one time series, if and only if the highest score is in the anomaly segment, this time series is considered to be detected.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventDetect.EventDetect.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Name Type Description <code>MetricInterface</code> <code>type[MetricInterface]</code> <p>An instance of Precision representing if the anomaly is detected.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventF1PA","title":"EventF1PA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventF1PA.EventF1PA","title":"EventF1PA","text":"<pre><code>EventF1PA(mode='log', base=3)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. </p> <p>One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventF1PA.EventF1PA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>A F1class (Evaluations.Metrics.F1class), including:</p> <p>best_f1: the value of best f1 score;</p> <p>precision: corresponding precision value;</p> <p>recall: corresponding recall value;</p> <p>threshold: the value of threshold when getting best f1.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthF1PA","title":"EventKthF1PA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventKthF1PA.EventKthF1PA","title":"EventKthF1PA","text":"<pre><code>EventKthF1PA(k, mode='log', base=3)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Defines the delay limit.</p> required <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthF1PA.EventKthF1PA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>A F1class (Evaluations.Metrics.F1class), including:</p> <p>best_f1: the value of best f1 score;</p> <p>precision: corresponding precision value;</p> <p>recall: corresponding recall value;</p> <p>threshold: the value of threshold when getting best f1.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthPrcPA","title":"EventKthPrcPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventKthPrcPA.EventKthPrcPA","title":"EventKthPrcPA","text":"<pre><code>EventKthPrcPA(k, mode='log', base=3)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Defines the delay limit.</p> required <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. </p> <p>One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthPrcPA.EventKthPrcPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auprc instance (Evaluations.Metrics.Auprc), including:</p> <p>auprc: auprc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthRocPA","title":"EventKthRocPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventKthRocPA.EventKthRocPA","title":"EventKthRocPA","text":"<pre><code>EventKthRocPA(k, mode='log', base=3)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Defines the delay limit.</p> required <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. </p> <p>One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventKthRocPA.EventKthRocPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auroc instance (Evaluations.Metrics.Auroc), including:</p> <p>auroc: auroc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventPrcPA","title":"EventPrcPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventPrcPA.EventPrcPA","title":"EventPrcPA","text":"<pre><code>EventPrcPA(\n    mode=\"log\", base=3, figname=None\n)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. </p> <p>One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventPrcPA.EventPrcPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auprc instance (Evaluations.Metrics.Auprc), including:</p> <p>auprc: auprc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.EventRocPA","title":"EventRocPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.EventRocPA.EventRocPA","title":"EventRocPA","text":"<pre><code>EventRocPA(\n    mode=\"log\", base=3, figname=None\n)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Defines the scale at which the anomaly segment is processed. </p> <p>One of:</p> <pre><code>- 'squeeze': View an anomaly event lasting t timestamps as one timepoint.\n- 'log': View an anomaly event lasting t timestamps as log(t) timepoint.\n- 'sqrt': View an anomaly event lasting t timestamps as sqrt(t) timepoint.\n- 'raw': View an anomaly event lasting t timestamps as t timepoint.\n</code></pre> <p>If using 'log', you can specify the param \"base\" to return the logarithm of x to the given base,  calculated as log(x) / log(base).</p> <code>'log'</code> <code>base</code> <code>int</code> <p>Default is 3.</p> <code>3</code>"},{"location":"API/Protocols/#Evaluations.Protocols.EventRocPA.EventRocPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auroc instance (Evaluations.Metrics.Auroc), including:</p> <p>auroc: auroc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointAuprcPA","title":"PointAuprcPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointAuprcPA.PointAuprcPA","title":"PointAuprcPA","text":"<pre><code>PointAuprcPA()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using Point-based point-adjustment Auprc to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointAuprcPA.PointAuprcPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auprc instance (Evaluations.Metrics.Auprc), including:</p> <p>auprc: auprc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointAurocPA","title":"PointAurocPA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointAurocPA.PointAurocPA","title":"PointAurocPA","text":"<pre><code>PointAurocPA(figname=None)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using Point-based point-adjustment Auroc to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointAurocPA.PointAurocPA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auroc instance (Evaluations.Metrics.Auroc), including:</p> <p>auroc: auroc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointF1","title":"PointF1","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointF1.PointF1","title":"PointF1","text":"<pre><code>PointF1()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using Traditional F1 score to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointF1.PointF1.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>A F1class (Evaluations.Metrics.F1class), including:</p> <p>best_f1: the value of best f1 score;</p> <p>precision: corresponding precision value;</p> <p>recall: corresponding recall value;</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointF1PA","title":"PointF1PA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointF1PA.PointF1PA","title":"PointF1PA","text":"<pre><code>PointF1PA()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using Point-based point-adjustment F1 score to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointF1PA.PointF1PA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>A F1class (Evaluations.Metrics.F1class), including:</p> <p>best_f1: the value of best f1 score;</p> <p>precision: corresponding precision value;</p> <p>recall: corresponding recall value;</p> <p>threshold: the value of threshold when getting best f1.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointKthF1PA","title":"PointKthF1PA","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointKthF1PA.PointKthF1PA","title":"PointKthF1PA","text":"<pre><code>PointKthF1PA(k)\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using Point-based point-adjustment F1 score to evaluate the models under k-delay strategy.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointKthF1PA.PointKthF1PA.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>A F1class (Evaluations.Metrics.F1class), including:</p> <p>best_f1: the value of best f1 score;</p> <p>precision: corresponding precision value;</p> <p>recall: corresponding recall value;</p> <p>threshold: the value of threshold when getting best f1.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointPrc","title":"PointPrc","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointPrc.PointPrc","title":"PointPrc","text":"<pre><code>PointPrc()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using traditional Auprc to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointPrc.PointPrc.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auprc instance (Evaluations.Metrics.Auprc), including:</p> <p>auprc: auprc value.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointRoc","title":"PointRoc","text":""},{"location":"API/Protocols/#Evaluations.Protocols.PointRoc.PointRoc","title":"PointRoc","text":"<pre><code>PointRoc()\n</code></pre> <p>             Bases: <code>EvalInterface</code></p> <p>Using traditional Auroc to evaluate the models.</p>"},{"location":"API/Protocols/#Evaluations.Protocols.PointRoc.PointRoc.calc","title":"calc","text":"<pre><code>calc(scores, labels, margins)\n</code></pre> <p>Returns:</p> Type Description <code>type[MetricInterface]</code> <p>An Auroc instance (Evaluations.Metrics.Auroc), including:</p> <p>auroc: auroc value.</p>"},{"location":"API/Summary/","title":"EasyTSAD.Summary","text":""},{"location":"API/Summary/#Summary.Summary","title":"Summary","text":"<pre><code>Summary()\n</code></pre>"},{"location":"API/Summary/#Summary.Summary.plot_aggreY","title":"plot_aggreY","text":"<pre><code>plot_aggreY(\n    types,\n    datasets,\n    methods,\n    training_schema,\n)\n</code></pre> <p>Plots aggregated anomaly scores for the specified types, datasets, methods, and training schema.</p> <p>Parameters:</p> Name Type Description Default <code>types</code> <code>str</code> <p>Types of the data.</p> required <code>datasets</code> <code>list</code> <p>List of dataset names.</p> required <code>methods</code> <code>list</code> <p>List of method names.</p> required <code>training_schema</code> <code>str</code> <p>Training schema name.</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"API/Summary/#Summary.Summary.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    datasets,\n    methods,\n    training_schema,\n    eval_items,\n)\n</code></pre> <p>Generates a CSV file based on the provided datasets, methods, training schema, and evaluation items.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>list</code> <p>List of dataset names.</p> required <code>methods</code> <code>list</code> <p>List of method names.</p> required <code>training_schema</code> <code>str</code> <p>Training schema name.</p> required <code>eval_items</code> <code>list</code> <p>List of evaluation items, where each item is a list containing the path to the final value in Eval JSONs,  e.g. [         [\"event-based f1 under pa with mode log\", \"f1\"],         [\"best f1 under pa\", \"f1\"]     ]</p> required <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the JSON file for a specific method, dataset, and training schema does not exist.</p>"},{"location":"API/TSADController/","title":"EasyTSAD.TSADController","text":""},{"location":"API/TSADController/#Controller.TSADController.TSADController","title":"TSADController","text":"<pre><code>TSADController(\n    cfg_path=None,\n    log_path=None,\n    log_level=\"info\",\n)\n</code></pre> <p>TSADController class represents a controller that manages global configuration and logging.</p> <p>Attributes:</p> Name Type Description <code>summary</code> <code>Summary</code> <p>summary the trained results, including generating CSV and aggregating all methods' anomaly scores on specific curve in one plot.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>Path to the configuration file. If provided, the configuration will be applied from this file. Defaults to None (Not Recommanded).</p> <code>None</code> <code>log_path</code> <code>str</code> <p>Path to the log file. If not provided, a default log file named \"TSADEval.log\" will be built in current workspace. Defaults to None.</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Log level to set for the logger. Options: \"debug\", \"info\", \"warning\", \"error\". Defaults to \"info\".</p> <code>'info'</code>"},{"location":"API/TSADController/#Controller.TSADController.TSADController.apply_cfg","title":"apply_cfg","text":"<pre><code>apply_cfg(path=None)\n</code></pre> <p>Applies configuration from a file.</p> <p>This method reads a configuration file from the specified path and overrides the corresponding default values.</p> NOTE <p>If no path is provided, it uses a default configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the configuration file. If None, default configuration is used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"API/TSADController/#Controller.TSADController.TSADController.do_evals","title":"do_evals","text":"<pre><code>do_evals(method, training_schema)\n</code></pre> <p>Performing evaluations based on saved anomaly scores. The result will be saved in Results/Evals, including the detailed evaluation results and the average evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method being used.</p> required <code>training_schema</code> <code>str</code> <p>The training schema being used.</p> required"},{"location":"API/TSADController/#Controller.TSADController.TSADController.plots","title":"plots","text":"<pre><code>plots(\n    method,\n    training_schema,\n    plot_yhat=111,\n)\n</code></pre> <p>Generate plots for the specified method and training schema. The plots are located in Results/Plots.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Method name.</p> required <code>training_schema</code> <code>str</code> <p>Training schema name.</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"API/TSADController/#Controller.TSADController.TSADController.run_exps","title":"run_exps","text":"<pre><code>run_exps(\n    method,\n    training_schema,\n    cfg_path=None,\n    diff_order=None,\n    preprocess=None,\n    hparams=None,\n)\n</code></pre> <p>Run experiments using the specified method and training schema.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method being used.</p> required <code>training_schema</code> <code>str</code> <p>The training schema being used.</p> required <code>cfg_path</code> <code>str</code> <p>Path to a custom configuration file. Defaults to None.</p> <code>None</code> <code>diff_order</code> <code>int</code> <p>The differential order. Defaults to None.</p> <code>None</code> <code>preprocess</code> <code>str</code> <p>The preprocessing method. Options: \"raw\", \"min-max\", \"z-score\". Defaults to None (equals to \"raw\"). </p> <code>None</code> <code>hparams</code> <code>dict</code> <p>Hyperparameters for the model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified training schema is not one of \"one_by_one\", \"all_in_one\", or \"zero_shot\".</p>"},{"location":"API/TSADController/#Controller.TSADController.TSADController.set_dataset","title":"set_dataset","text":"<pre><code>set_dataset(\n    datasets,\n    dirname=None,\n    dataset_type=\"UTS\",\n    specify_curves=False,\n    curve_names=None,\n    train_proportion=None,\n    valid_proportion=None,\n)\n</code></pre> <p>Registers the dataset settings and related parameters for the TSADController instance. This will check if the paths and the parameters are valid.</p> NOTE <p>If you want to run all curves in the dataset, set \"specify_curves\" to False. In this mode, \"curve_names\" should be set to None.</p> <p>Otherwise, if you want to specify some time series in a dataset, please specify ONLY ONE dataset and the curves in this dataset. E.g. set_dataset(datasets=\"WSD\", \"curve_names\"=[\"1\", \"2\"])</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[str, list[str]]</code> <p>Name(s) of the dataset(s) to be set. Can be a single string or a list of strings.</p> required <code>dirname</code> <code>str</code> <p>Path to the dataset directory. If not provided, it will be fetched from the configuration file. Defaults to None.</p> <code>None</code> <code>curve_type</code> <code>str</code> <p>Type of the datasets. Defaults to \"UTS\".</p> required <code>specify_curves</code> <code>bool</code> <p>Flag indicating whether to specify individual curves within the dataset(s). Defaults to False.</p> <code>False</code> <code>curve_names</code> <code>Union[None, str, list[str]]</code> <p>Name(s) of the curve(s) to be used. Can be None, a single string, or a list of strings. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset directory path is not specified.</p> <code>FileNotFoundError</code> <p>If the dataset directory or any of the specified datasets or curves do not exist.</p> <p>Returns:</p> Type Description <p>None</p>"},{"location":"API/TSADController/#Controller.TSADController.TSADController.set_evals","title":"set_evals","text":"<pre><code>set_evals(evals)\n</code></pre> <p>Registers the evaluation protocols used for performance evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>evals</code> <code>list[EvalInterface]</code> <p>The evaluation instances inherited from EvalInterface.</p> required"},{"location":"API/TSData/","title":"TSData","text":"<p>TSDataSet read data and provides various method for preprocessing data in datasets, including MinMax normalization and Z-score normalization.</p> <p>There may be some missing values and inconsecutive timestamps in some datasets. We complement the missing timestamps to make it continuous at the possible minimum intervals, meanwhile filling the n/a values  using the linear interpolation method.</p> <p>UTS:   AIOPS: 29 curves   NAB: 10 curves   WSD: 210 curves   Yahoo: 367 curves</p>"},{"location":"API/TSData/#DataFactory.TSData.TSData","title":"TSData","text":"<pre><code>TSData(\n    train,\n    valid,\n    test,\n    train_label,\n    test_label,\n    valid_label,\n    info,\n)\n</code></pre> <p>TSData contains all information used for training, validation and test, including the dataset values and dataset information. Some typical preprocessing method are provided in class methods.</p> <p>Attributes:</p> Name Type Description <code>train</code> <code>ndarray</code> <p>The training set in numpy format;</p> <code>valid</code> <code>ndarray</code> <p>The validation set in numpy format;</p> <code>test</code> <code>ndarray</code> <p>The test set in numpy format;</p> <code>train_label</code> <code>ndarray</code> <p>The labels of training set in numpy format;</p> <code>test_label</code> <code>ndarray</code> <p>The labels of test set in numpy format;</p> <code>valid_label</code> <code>ndarray</code> <p>The labels of validation set in numpy format;</p> <code>info</code> <code>dict</code> <p>Some informations about the dataset, which might be useful.</p>"},{"location":"API/TSData/#DataFactory.TSData.TSData.buildfrom","title":"buildfrom  <code>classmethod</code>","text":"<pre><code>buildfrom(\n    types,\n    dataset,\n    data_name,\n    train_proportion=1,\n    valid_proportion=0,\n)\n</code></pre> <p>Build customized TSDataSet instance from numpy file.</p> <p>Parameters:</p> Name Type Description Default <code>types</code> <code>str</code> <p>The dataset type. One of \"UTS\" or \"MTS\";</p> required <code>dataset</code> <code>str</code> <p>The dataset name where the curve comes from, e.g. \"WSD\";</p> required <code>dataname</code> <code>str</code> <p>The curve's name (Including the suffix '.npy'), e.g. \"1.npy\";</p> required <p>Returns:</p> <p>A TSDataSet instance.</p>"},{"location":"API/TSData/#DataFactory.TSData.TSData.min_max_norm","title":"min_max_norm","text":"<pre><code>min_max_norm(feature_range=(0, 1))\n</code></pre> <p>Function to preprocess one metric using Min Max Normalization and generate training set, validation set and test set according to your settings. Then the datas are cliped to [feature_range.min - 1, feature_range.max + 1]</p> <p>Params:</p> <p>feature_range - tuple (min, max), default=(0, 1)   Desired range of transformed data.</p>"},{"location":"API/TSData/#DataFactory.TSData.TSData.z_score_norm","title":"z_score_norm","text":"<pre><code>z_score_norm()\n</code></pre> <p>Function to preprocess one metric using Standard (Z-score) Normalization and generate training set, validation set and test set.</p>"}]}